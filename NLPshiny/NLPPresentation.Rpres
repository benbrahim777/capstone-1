Predicting next word with N-grams
========================================================
author: Pablo Arias
date: 4/17/2015

John Hopkins University Capstone Project

Shiny App to predict next word based on previously typed
words

Data Used and Pre-processing
========================================================

The data was provided as part of this course. 

- Data was merged and 20% of it was split into 80% training 
and 20% data sets
- Numbers, punctuation, long and bad words where removed
- After measuring perplexity, only 200K entries were used
- Words with occurrence smaller than 5 were replaced with
"Unknown"" token
- Final word count of 36K was used
- A stemmed version of the words was created to reduce
number of N-grams
- N-grams were generated by replacing words with indices for
efficiency (uni-gram to 5-grams)

The NLP Model
========================================================

- A combination of Good-Turing and Kneser-Ney smoothing was used.
- Good-Turing smoothing as proposed by *William A. Gale* 
[here](http://www.d.umn.edu/~tpederse/Courses/CS8761-FALL02/Code/sgt-gale.pdf) to address sparsity.
- Regular Kneser-Ney smoothing as proposed by *Chen and Goodman* and
described by *Frankie James* [here](http://www.riacs.edu/navroot/Research/TR_pdf/TR_00.07.pdf)
was used as the probabilistic model, using Pcontinuation probability for
the lowest order N-gram
- 5-grams were used for marginal perplexity improvement, 
but did not affect performance significantly.
- R datatable packaged used for fast index search.
- Words are stemmed before running through predictor
- As words are being typed the uni-gram is used to predict the
word being typed
- Top 3 predicted words are offered

The Shiny App
========================================================

Application can be accessed [here](https://pabloarias.shinyapps.io/NLPshiny/)

- Left pane with brief instructions
- Center pane with resizeable text area for input text to be predicted
- Three buttons with the top three predictions. The blue is the
prediction with highest probability
- Pressing the button will complete the word being typed or 
will add the next word
- Pressing <SPACE> while typing a word will complete with the
prediction with highest probability
- Right pane with some statistics for the session

Future Improvements
========================================================

- Running word list against spell-checker before generating model
- Try to split words from original corpus that might have joined
during punctuation removal
- Use other data sources
- Use a modified Kneser-Ney algorithm as proposed by *Frankie James* [here](http://www.riacs.edu/navroot/Research/TR_pdf/TR_00.07.pdf)
- Have the algorithm adapt to the user's text input style
- Use an additional model that ignores stop words and reduce
distance between meaningful words for better context based
prediction
- Better handling when only one or two words are predicted
